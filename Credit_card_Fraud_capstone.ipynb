{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Description\n",
        "#Prediction of Credit Card fraud:\n",
        "\n",
        "A credit card is one of the most used financial products to make online purchases and payments. Though the Credit cards can be a convenient way to manage your finances, they can also be risky. Credit card fraud is the unauthorized use of someone else's credit card or credit card information to make purchases or withdraw cash.\n",
        "\n",
        "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase. \n",
        "\n",
        "The dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "We have to build a classification model to predict whether a transaction is fraudulent or not.\n"
      ],
      "metadata": {
        "id": "AAS9x2Py95XI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reading the data"
      ],
      "metadata": {
        "id": "-uhsvhF_91PD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "euFBvuXMgmVW",
        "outputId": "eea67bfd-ff07-41d9-92a6-97ae2865a120"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d78be156276a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/creditcard.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 31 fields in line 1987, saw 43\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df= pd.read_csv(\"/content/creditcard.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "pQEbW3Ys-IJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data is preprocessed. Most columns have been processed using PCA, except for Time, Amount and Class.\n",
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "6O4hOM5vhnHV",
        "outputId": "7acd187b-d4dc-44b8-fa01-f71085ea4ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4134a0a16831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Data is preprocessed. Most columns have been processed using PCA, except for Time, Amount and Class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Data ranges differ greatly amongst all features. Normalization should be done.\n",
        "- Mean and median are similar in most of the features which shows symmetrical distribution.\n",
        "- Info shows no null data and same data type across all features except Class."
      ],
      "metadata": {
        "id": "1Z2-mLKZ-iUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "gTLaIlKdhtyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Class.unique()"
      ],
      "metadata": {
        "id": "pLLlh9_5i46L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "corr_matrix= df.corr().round(1)\n",
        "sns.heatmap(data= corr_matrix, annot=True, linewidths=0.5, square=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HLP00yUCjzLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Empty values\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "9ouaCMHipAJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Boxplot for all the variable to identify ranges and outliers"
      ],
      "metadata": {
        "id": "OCTZemmj2y0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_dic= {}\n",
        "for k,v in enumerate(df.columns):\n",
        "  col_dic[v]= k+1"
      ],
      "metadata": {
        "id": "6hx17rS6pWJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(22,28))\n",
        "\n",
        "for variable, i in col_dic.items():\n",
        "  plt.subplot(16,2,i, axisbelow= True)\n",
        "  sns.boxplot(x= df[variable])\n",
        "# set the spacing between subplots\n",
        "plt.subplots_adjust(hspace=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Bd9R6yjRpxUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takeaways:\n",
        "- All variables contain high amount of outliers\n",
        "- All variables have a median close to 0\n",
        "- Distribution on all the variables appear to be normal with small variations in V1(Positive skewed) and V3 (negative skewed)\n",
        "- All columns have different ranges, comparison is better after normalization to determine dispersion"
      ],
      "metadata": {
        "id": "uqAWUM0BvoXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking Distribution"
      ],
      "metadata": {
        "id": "wS7b460e228X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def centers(data):\n",
        "  mean,median, modes= data.mean(), data.median(), data.mode()\n",
        "  mn= plt.axvline(mean, color=\"red\")\n",
        "  md= plt.axvline(median, color=\"green\")   \n",
        "  for mode in modes:\n",
        "    mo= plt.axvline(mean, color=\"yellow\")\n",
        "  plt.legend((mn,md,mo), \"Mean Median Mode\".split())"
      ],
      "metadata": {
        "id": "0zU1IwYP35eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(df.V1)\n",
        "centers(df.V1)"
      ],
      "metadata": {
        "id": "PaSJwFkD2w4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(df.Amount)\n",
        "centers(df.Amount)"
      ],
      "metadata": {
        "id": "gip6Q6LdFyU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a subplot with multiple plots\n",
        "fig, axs = plt.subplots(8, 4, figsize=(20, 20))\n",
        "axs = axs.ravel()\n",
        "\n",
        "# Plot histograms for all columns\n",
        "for i, column in enumerate(df.columns):\n",
        "    sns.histplot(df[column], ax=axs[i])\n",
        "    centers(df[column])\n",
        "    axs[i].set_title(column)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7DVvAt0HO9qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check class balance:"
      ],
      "metadata": {
        "id": "5Xy-i4E3EZfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Class'].value_counts()"
      ],
      "metadata": {
        "id": "MTKPcJksySIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imbalanced Dataset: 0 non.fraudulent and 1 fraudulent\n",
        "#The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.countplot(x= \"Class\", data= df)\n",
        "plt.title (\"Fraudulent Transactions Vs. Non Fraudulent Transactions\")\n",
        "plt.xlabel (\"Fraud\")\n",
        "plt.ylabel (\"Non Fraud\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NWx6DZZV59mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check and process Outliers\n",
        "-Outliers influence the best fit line. Check the amount of outliers and depending on percentage either transform them, drop them or leave them.\n"
      ],
      "metadata": {
        "id": "pLQX9i2rE04K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#Tukey Method:\n",
        "for variable in col_dic.keys():\n",
        "  #q75,q25=np.percentile(df[variable], [75,25])\n",
        "  q25 = df[variable].quantile(0.25)\n",
        "  q75 = df[variable].quantile(0.75)\n",
        "  iqr=q75-q25\n",
        "  min_threshold= q25-(iqr*1.5)\n",
        "  max_threshold= q75+(iqr*1.5)\n",
        "  outliers = df[(df[variable] < min_threshold) | (df[variable] > max_threshold)]\n",
        "  num_outliers = outliers.shape[0]\n",
        "  percentage= num_outliers/284807*100\n",
        "  print(\"Number of outliers and percentage of it in {}: {} and {:0.2f}% \\n\".format(variable, num_outliers, percentage))\n",
        "  "
      ],
      "metadata": {
        "id": "ZWkobJm8E27a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def replace_outliers(df, col_name, k=1.5):\n",
        "    q1 = df[col_name].quantile(0.25)\n",
        "    q3 = df[col_name].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - k*iqr\n",
        "    upper_bound = q3 + k*iqr\n",
        "    \n",
        "    df[col_name] = np.where(df[col_name] < lower_bound, lower_bound, df[col_name])\n",
        "    df[col_name] = np.where(df[col_name] > upper_bound, upper_bound, df[col_name])\n",
        "      \n",
        "    return df"
      ],
      "metadata": {
        "id": "UwkLWRk3-fJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in col_dic.keys():\n",
        "  if i != \"Class\":\n",
        "    replace_outliers(df, i, k=1.5)"
      ],
      "metadata": {
        "id": "nQz19e-AA9wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "d6ESp_YXA5wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Before Replacing outliers\n",
        "sns.boxplot(df[\"V1\"],orient=\"horizontal\")"
      ],
      "metadata": {
        "id": "FriGlHHzOceN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#After replacing outliers\n",
        "sns.boxplot(x=df[\"V1\"])"
      ],
      "metadata": {
        "id": "f6iMOga1OpRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info() #Check null values again"
      ],
      "metadata": {
        "id": "Adec0qOkWpz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scaler\n",
        "- Dataset (V1...) were previously processed using PCA, therefore I apply StandardScaler to the remaining features (Time and Amount)\n",
        "\n"
      ],
      "metadata": {
        "id": "c8GP_6n2XO0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Scale 'Time' and 'Amount'\n",
        "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n",
        "df['Time'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "QYZCZXMlXS0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "tsRVDcBBYfgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum().sum()"
      ],
      "metadata": {
        "id": "h3kQQzie8ewA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dealing with NaNs"
      ],
      "metadata": {
        "id": "MdWmG3fb95cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna(df.mean())\n",
        "df.isnull().sum().sum()"
      ],
      "metadata": {
        "id": "XYFEwbXX948z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to this point, the dataset has been analysed, processed by dealing with outliers and standarizing the data. Next, I will create datasets using undersampling and oversampling to test with different ML or DL models."
      ],
      "metadata": {
        "id": "HOvPSn5pZs_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dealing with imbalanced dataset:\n",
        "- I test three datasets:\n",
        "  1. Undersampling the majority class\n",
        "  2. Oversampling the minority class\n",
        "  3. SMOTE\n",
        "I use the imblearn library"
      ],
      "metadata": {
        "id": "WRBFYxOlaMYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imblearn"
      ],
      "metadata": {
        "id": "wBMJmmF1ZGxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= df.drop([\"Class\"], axis=1)\n",
        "y= df.Class"
      ],
      "metadata": {
        "id": "Qf9X1u9xcj1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fraud= len(df[df[\"Class\"]==1])\n",
        "no_fraud= len(df[df[\"Class\"]==0])"
      ],
      "metadata": {
        "id": "baTCzsELbGPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Fraudulent transactions: {fraud}\")\n",
        "print(f\"Non Fraudulent transactions: {no_fraud}\")"
      ],
      "metadata": {
        "id": "k4M1g4fPbZ3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Undersampling\n",
        "Testing two techniques:\n",
        "- Edited NearestNeighbours and RandomOverSampler"
      ],
      "metadata": {
        "id": "gPPJvRMMbDdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import CondensedNearestNeighbour,EditedNearestNeighbours,NearMiss,NeighbourhoodCleaningRule,OneSidedSelection,RandomUnderSampler,TomekLinks"
      ],
      "metadata": {
        "id": "NLcKAzpGbChN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomUnderSampler"
      ],
      "metadata": {
        "id": "ysQfyRgbc7ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rus = RandomUnderSampler(random_state=0, replacement=True)\n",
        "X_random_undersampled, y_random_undersampled = rus.fit_resample(X, y)\n",
        "X_random_undersampled.head()"
      ],
      "metadata": {
        "id": "1i5YaUecc6g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_random_undersampled))\n",
        "print(len(y_random_undersampled))"
      ],
      "metadata": {
        "id": "VD-s21r6eKOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y_random_undersampled)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eBT41cqvdc2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EditedNearestNeighbours\n",
        "- kind_sel='all' will be less conservative than kind_sel='mode', and more samples will be excluded in the former strategy than the latest."
      ],
      "metadata": {
        "id": "d_qRrWzwc7jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enn = EditedNearestNeighbours(kind_sel=\"all\")\n",
        "X_edited_undersampled, y_edited_undersampled = rus.fit_resample(X, y)\n",
        "X_edited_undersampled.head()"
      ],
      "metadata": {
        "id": "igB0udLtebig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_edited_undersampled))\n",
        "print(len(y_edited_undersampled))"
      ],
      "metadata": {
        "id": "qfGF43AUe_7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y_edited_undersampled)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cAB2J0_EfFUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##OverSampling\n",
        " I use two techniques\n",
        "  - Adasyn and RandomOverSampler\n",
        "\n",
        "RandomOverSampler is over-sampling by duplicating some of the original samples of the minority class, SMOTE and ADASYN generate new samples in by interpolation."
      ],
      "metadata": {
        "id": "oAx-dMJ9fJn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from imblearn.over_sampling import BorderlineSMOTE,ADASYN,KMeansSMOTE,SMOTE,RandomOverSampler,SVMSMOTE"
      ],
      "metadata": {
        "id": "oJOWzSuufacK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomOverSampler"
      ],
      "metadata": {
        "id": "TobuxSDWfqGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ros = RandomOverSampler(random_state=0)\n",
        "X__random_oversampled, y_random_oversampled = ros.fit_resample(X, y)\n",
        "X__random_oversampled.head() "
      ],
      "metadata": {
        "id": "lZZOzQAVfgwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X__random_oversampled))\n",
        "print(len(y_random_oversampled))"
      ],
      "metadata": {
        "id": "QUPtA1bpgD_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y_random_oversampled)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6xDol2TXgHZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adasyn Oversampler\n"
      ],
      "metadata": {
        "id": "fpEwRWy9fqzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_adasyn_oversampled, y_adasyn_oversampled = ADASYN().fit_resample(X, y)\n",
        "X_adasyn_oversampled.head() "
      ],
      "metadata": {
        "id": "6T1qFRKUfrBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_adasyn_oversampled))\n",
        "print(len(y_adasyn_oversampled)) "
      ],
      "metadata": {
        "id": "nvyup_RBgE2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y_adasyn_oversampled)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D3MuJlWJgIPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE\n",
        "- the Synthetic Minority Oversampling Technique (SMOTE)"
      ],
      "metadata": {
        "id": "jmFxvomPgru6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_smote_oversampled, y_smote_oversampled = SMOTE().fit_resample(X, y)\n",
        "X_smote_oversampled.head() "
      ],
      "metadata": {
        "id": "oyrK3vTXhFnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_smote_oversampled))\n",
        "print(len(y_smote_oversampled))"
      ],
      "metadata": {
        "id": "UpA3kcJZhPju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split all the datasets into train and test:"
      ],
      "metadata": {
        "id": "KXHYTJNes6Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#Original\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state= 42)\n",
        "\n",
        "#Undersampling\n",
        "X_train_ru, X_test_ru, y_train_ru, y_test_ru = train_test_split(X_random_undersampled, y_random_undersampled, random_state= 42)\n",
        "X_train_eu, X_test_eu, y_train_eu, y_test_eu = train_test_split(X_edited_undersampled, y_edited_undersampled, random_state= 42)\n",
        "\n",
        "#Oversampling\n",
        "X_train_ro, X_test_ro, y_train_ro, y_test_ro = train_test_split(X__random_oversampled, y_random_oversampled, random_state= 42)\n",
        "X_train_ao, X_test_ao, y_train_ao, y_test_ao = train_test_split(X_adasyn_oversampled, y_adasyn_oversampled, random_state= 42)\n",
        "\n",
        "X_train_so, X_test_so, y_train_so, y_test_so = train_test_split(X_smote_oversampled, y_smote_oversampled, random_state= 42)"
      ],
      "metadata": {
        "id": "Q29FCFbxs2fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following dictionary is used for automating training:\n"
      ],
      "metadata": {
        "id": "wgUFfYSBuiQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_aug= {\"original\": [X_train, X_test, y_train, y_test],\n",
        "           \"Random Undersampling\": [X_train_ru, X_test_ru, y_train_ru, y_test_ru],\n",
        "           \"Edited Undersampling\":[X_train_eu, X_test_eu, y_train_eu, y_test_eu],\n",
        "           \"Random Oversampling\": [X_train_ro, X_test_ro, y_train_ro, y_test_ro],\n",
        "           \"Adasyn Oversampling\": [X_train_ao, X_test_ao, y_train_ao, y_test_ao],\n",
        "           \"Smote Oversampling\": [X_train_so, X_test_so, y_train_so, y_test_so]}"
      ],
      "metadata": {
        "id": "B6k6Gbgyuhiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "073R4kCxAbsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model training\n",
        "- I will test with RandomForest and a simple NN\n",
        "- Comparison of the Original, Undersampled, and Oversampled datasets\n",
        "- Later hyperparameter tuning will be performed in both for the best scoring dataset.\n"
      ],
      "metadata": {
        "id": "kTvt_bzvny2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conf_matrix(confusion_matrix):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    f, ax= plt.subplots()\n",
        "    labels= y_test.unique()\n",
        "    sns.heatmap(confusion_matrix, cmap=\"Blues\", annot = True, xticklabels=labels, yticklabels=labels);\n",
        "    plt.xlabel(\"Predicted\", fontsize=20)\n",
        "    plt.ylabel(\"Actual\", fontsize=20)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DS1sqld6sRaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history, name):\n",
        "  hist=history.history\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.title(name)\n",
        "  plt.plot(history.epoch, hist[\"accuracy\"], label=\"Train Acuraccy\")  \n",
        "  plt.plot(history.epoch, hist[\"val_accuracy\"], label=\"Val Acuraccy\")\n",
        "  plt.legend([\"Training\", \"Validation\"], loc=\"best\")\n"
      ],
      "metadata": {
        "id": "n-P7Z2hJ2YvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_ml(model, name, dataset):\n",
        "  model.fit(dataset[0], dataset[2])\n",
        "  y_pred=model.predict(dataset[1])\n",
        "  accuracy= accuracy_score(dataset[3],y_pred)\n",
        "  matrix= confusion_matrix(dataset[3],y_pred)\n",
        "  print(f\"The model name is: {name}. It's accuracy is: {accuracy} \\n\")\n",
        "  conf_matrix(matrix)\n",
        "  return accuracy\n",
        "  "
      ],
      "metadata": {
        "id": "TsYYI0UV5aSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier"
      ],
      "metadata": {
        "id": "b3akd80YrcYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "asB3dgHRnyTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc= RandomForestClassifier()"
      ],
      "metadata": {
        "id": "5v4TGWpAF94p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_results= {}\n",
        "for k,v in datasets_aug.items():\n",
        "  acc= training_ml(rfc, k,v)\n",
        "  dic_results[k] = acc\n",
        "  "
      ],
      "metadata": {
        "id": "Fmq85aqJ6sBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_results"
      ],
      "metadata": {
        "id": "HKn8-c6GG02u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml= pd.DataFrame(list(dic_results.items()), columns=[\"Model\", \"Accuracy\"])"
      ],
      "metadata": {
        "id": "N8FMgWHmHoYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml"
      ],
      "metadata": {
        "id": "h77e7NiDkd8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network\n"
      ],
      "metadata": {
        "id": "t90YVEnzrcij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, callbacks\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, InputLayer, Dense, Conv2D, Flatten, MaxPooling2D, BatchNormalization, Dropout, RandomFlip, RandomRotation, RandomZoom, InputLayer, Rescaling, Resizing, GlobalAveragePooling2D \n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from keras.utils import plot_model\n"
      ],
      "metadata": {
        "id": "EgllguNBzpoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# define the input shape\n",
        "input_shape = (X_train.shape[1],)\n",
        "\n",
        "# define the sequential model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=64, activation='relu', input_shape=input_shape),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "AAdgxEClOAA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define early stopping callback\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "def train_nn(model, name, dataset):\n",
        "  train_history= model.fit(\n",
        "    dataset[0],\n",
        "    dataset[2],\n",
        "    epochs=20,\n",
        "    verbose=2,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop]\n",
        "  )\n",
        "  score= model.evaluate(dataset[1], dataset[3], verbose=2)\n",
        "  print(f\"\\n Model: {name} \\n\")\n",
        "  print(\"Test Accuracy: {:0.2f} \\n\".format(score[1]*100))\n",
        "  plot_training_history(train_history, name)\n",
        "  return score\n",
        "\n"
      ],
      "metadata": {
        "id": "_47z1zDiP9O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_results_nn= {}\n",
        "for k,v in datasets_aug.items():\n",
        "  acc = train_nn(model, k,v)\n",
        "  dic_results_nn[k] = acc[1]\n"
      ],
      "metadata": {
        "id": "Zld3fFyV1Z4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_results_nn"
      ],
      "metadata": {
        "id": "1fSwOKicWcEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_nn= pd.DataFrame(list(dic_results.items()), columns=[\"Model\", \"Accuracy\"])\n",
        "df_nn"
      ],
      "metadata": {
        "id": "tPif6XZekDvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Best performing models:\n",
        "\n",
        "Random Forest: Random Undersampling.\n",
        "Oversampling seems to create near 100% accuracy but also overfitting. Same as the original dataset.\n",
        "\n",
        "Neural Network: Random Undersampling\n",
        "Same as with the Random forest classifier, the use of oversampling obtained 99.8 and above accuracy. It might be a clear case of overfitting. Therefore, choosing the best next option.\n",
        "\n",
        "For both the best option is the dataset processed with Random Undersampling. This will be the dataset used for futher hyperparameter tuning. "
      ],
      "metadata": {
        "id": "tl-MecgWkT1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "wjxxjrDWwVng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_rf={\n",
        "    \"criterion\": (\"gini\", \"entropy\"),\n",
        "    \"min_samples_leaf\": list(range(1,10)),\n",
        "    \"max_depth\": list(range(1,10))\n",
        "    }\n"
      ],
      "metadata": {
        "id": "JE2iBU6xlnkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "gs= GridSearchCV(rfc, params_rf, scoring= \"accuracy\", n_jobs=-1, cv=3, verbose=1)\n",
        "gs.fit(X_train_ru, y_train_ru)"
      ],
      "metadata": {
        "id": "KCGMo53JnOgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs.best_estimator_"
      ],
      "metadata": {
        "id": "HG6_ZtJooPTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs.best_params_"
      ],
      "metadata": {
        "id": "7kAmMROBoVpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_new= RandomForestClassifier(criterion = 'gini', max_depth= 6, min_samples_leaf = 2)\n",
        "\n",
        "model= training_ml(rfc_new, \"Random UnderSampling\",datasets_aug[\"Random Undersampling\"])"
      ],
      "metadata": {
        "id": "gK_LOjEHpGmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"rf_creditcard_fraud.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "#Load:\n",
        "\n",
        "#with open(\"model.pkl\", \"rb\") as f:\n",
        "#    model = pickle.load(f)"
      ],
      "metadata": {
        "id": "IibsYrE_uUGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network:"
      ],
      "metadata": {
        "id": "ERtamxDHqX9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_nn={\n",
        "    \"epochs\": list(range(10,30)),\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    \"optimizer\": [\"adam\", \"SDG\"],\n",
        "}"
      ],
      "metadata": {
        "id": "g7xkUoxhnKkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Define the neural network architecture\n",
        "def create_model(optimizer='sgd', activation='relu'):\n",
        "    # define the input shape\n",
        "    input_shape = (X_train.shape[1],)\n",
        "\n",
        "    # define the sequential model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(units=64, activation= activation, input_shape=input_shape),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=32, activation=activation),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=16, activation=activation),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(optimizer= optimizer,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "params_nn={\n",
        "    \"epochs\": list(range(10,30)),\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    \"optimizer\": [\"adam\", \"sgd\", \"AdamW\"] }\n",
        "\n",
        "# Create a Keras classifier\n",
        "keras_clf = KerasClassifier(build_fn=create_model, epochs=20, batch_size=32, verbose=0)\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=keras_clf, param_grid=params_nn, cv=3)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(datasets_aug[\"Random Undersampling\"][0], datasets_aug[\"Random Undersampling\"][2])\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "gf1OnhldsRBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (X_train.shape[1],)\n",
        "\n",
        "    # define the sequential model\n",
        "model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(units=64, activation= \"tanh\", input_shape=input_shape),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=32, activation=\"tanh\"),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=16, activation=\"tanh\"),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # compile the model\n",
        "model.compile(optimizer= \"adam\",\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "M89BABlDx-cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_history= model.fit(\n",
        "datasets_aug[\"Random Undersampling\"][0],\n",
        "datasets_aug[\"Random Undersampling\"][2],\n",
        "epochs=26,\n",
        "verbose=2,\n",
        "validation_split=0.1,\n",
        "callbacks=[early_stop]\n",
        ")\n",
        "score= model.evaluate(datasets_aug[\"Random Undersampling\"][1], datasets_aug[\"Random Undersampling\"][3], verbose=2)\n",
        "print(f\"\\n Model: Random Undersampling \\n\")\n",
        "print(\"Test Accuracy: {:0.2f} \\n\".format(score[1]*100))\n",
        "plot_training_history(train_history, \"Random Undersampling\")"
      ],
      "metadata": {
        "id": "_pGf4-ROrp6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('nn_credit_card_fraud_weights')\n",
        "model.save('nn_credit_card_fraud.h5')"
      ],
      "metadata": {
        "id": "GhGx0WCPtwfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}